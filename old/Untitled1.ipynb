{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPPK5Xt9WOT9hXLDkNhafMa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/icirauqui/TBot_RL/blob/main/Untitled1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nvv1w21ClubG",
        "outputId": "417e8403-2230-4cbc-8294-12dc3b14de64"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "!pip install stable-baselines[mpi]==2.10.0"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "Collecting stable-baselines[mpi]==2.10.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/fe/db8159d4d79109c6c8942abe77c7ba6b6e008c32ae55870a35e73fa10db3/stable_baselines-2.10.0-py3-none-any.whl (248kB)\n",
            "\u001b[K     |████████████████████████████████| 256kB 5.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (3.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (1.19.5)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (4.1.2.30)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (1.4.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (1.0.1)\n",
            "Requirement already satisfied: gym[atari,classic_control]>=0.11 in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (0.17.3)\n",
            "Requirement already satisfied: cloudpickle>=0.5.5 in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (1.3.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (1.1.5)\n",
            "Requirement already satisfied: mpi4py; extra == \"mpi\" in /tensorflow-1.15.2/python3.7 (from stable-baselines[mpi]==2.10.0) (3.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines[mpi]==2.10.0) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines[mpi]==2.10.0) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines[mpi]==2.10.0) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines[mpi]==2.10.0) (2.4.7)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (1.5.0)\n",
            "Requirement already satisfied: Pillow; extra == \"atari\" in /usr/local/lib/python3.7/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (7.1.2)\n",
            "Requirement already satisfied: atari-py~=0.2.0; extra == \"atari\" in /usr/local/lib/python3.7/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (0.2.9)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->stable-baselines[mpi]==2.10.0) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->stable-baselines[mpi]==2.10.0) (1.15.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (0.16.0)\n",
            "Installing collected packages: stable-baselines\n",
            "  Found existing installation: stable-baselines 2.2.1\n",
            "    Uninstalling stable-baselines-2.2.1:\n",
            "      Successfully uninstalled stable-baselines-2.2.1\n",
            "Successfully installed stable-baselines-2.10.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YhEsQkElPM6"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "df = pd.read_csv('data/BTCUSDT.csv')\n",
        "\n",
        "MAX_QUOTE_ASSET_VOLUME = df.loc[\n",
        "    df['Quote asset volume'].idxmax()]['Quote asset volume']\n",
        "\n",
        "MAX_NUMBER_of_TRADES = df.loc[\n",
        "    df['Number of trades'].idxmax()]['Number of trades']\n",
        "\n",
        "MAX_TAKER_BUY_BASE_ASSET_VOLUME = df.loc[\n",
        "    df['Taker buy base asset volume'].idxmax()]['Taker buy base asset volume']\n",
        "\n",
        "MAX_TAKER_BUY_QUOTE_ASSET_VOLUME = df.loc[df[\n",
        "    'Taker buy quote asset volume'].idxmax()]['Taker buy quote asset volume']\n",
        "\n",
        "MAX_ACCOUNT_BALANCE = 10000000\n",
        "INITIAL_ACCOUNT_BALANCE = 1000\n",
        "MAX_CRYPTO_PRICE = 20000\n",
        "MAX_CRYPTO = 21000000\n",
        "MAKER_FEE = 0.00075\n",
        "TAKER_FEE = 0.00075\n",
        "BNBUSDTHELD = 1000\n",
        "MAX_STEPS = int(os.getenv('MAX_STEPS', 400))"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULFIEtRAleYP"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import gym\n",
        "import random\n",
        "from gym import spaces\n",
        "\n",
        "\n",
        "class CryptoEnv(gym.Env):\n",
        "    def __init__(self, df, title=None):\n",
        "        self.df = df\n",
        "        self.reward_range = (-MAX_ACCOUNT_BALANCE,\n",
        "                             MAX_ACCOUNT_BALANCE)\n",
        "        self.total_fees = 0\n",
        "        self.total_volume_traded = 0\n",
        "        self.crypto_held = 0\n",
        "        self.bnb_usdt_held = BNBUSDTHELD\n",
        "        self.bnb_usdt_held_start = BNBUSDTHELD\n",
        "        self.episode = 1\n",
        "\n",
        "        # Graph to render\n",
        "        self.graph_reward = []\n",
        "        self.graph_profit = []\n",
        "        self.graph_benchmark = []\n",
        "\n",
        "        # Action space from -1 to 1, -1 is short, 1 is buy\n",
        "        self.action_space = spaces.Box(low=-1,\n",
        "                                       high=1,\n",
        "                                       shape=(1, ),\n",
        "                                       dtype=np.float16)\n",
        "        # Observation space contains only the actual price for the moment\n",
        "        self.observation_space = spaces.Box(low=0,\n",
        "                                            high=1,\n",
        "                                            shape=(10, 5),\n",
        "                                            dtype=np.float16)\n",
        "\n",
        "    def reset(self):\n",
        "        self.balance = INITIAL_ACCOUNT_BALANCE\n",
        "        self.net_worth = INITIAL_ACCOUNT_BALANCE + BNBUSDTHELD\n",
        "        self.max_net_worth = INITIAL_ACCOUNT_BALANCE\n",
        "        BNBUSDTHELD\n",
        "        self.total_fees = 0\n",
        "        self.total_volume_traded = 0\n",
        "        self.crypto_held = 0\n",
        "        self.bnb_usdt_held = BNBUSDTHELD\n",
        "        self.episode_reward = 0\n",
        "\n",
        "        # Set the current step to a random point within the data frame\n",
        "        # Weights of the current step follow the square function\n",
        "        start = list(range(4, len(self.df.loc[:, 'Open'].values) - MAX_STEPS)) + self.df.index[0]\n",
        "        weights = [i for i in start]\n",
        "        self.current_step = random.choices(start, weights)[0]\n",
        "        self.start_step = self.current_step\n",
        "\n",
        "        return self._next_observation()\n",
        "\n",
        "    def _next_observation(self):\n",
        "        # Get the data for the last 5 timestep\n",
        "        frame = np.array([\n",
        "            self.df.loc[self.current_step - 4:self.current_step, 'Open'],\n",
        "            self.df.loc[self.current_step - 4:self.current_step, 'High'],\n",
        "            self.df.loc[self.current_step - 4:self.current_step, 'Low'],\n",
        "            self.df.loc[self.current_step - 4:self.current_step, 'Close'],\n",
        "            self.df.loc[self.current_step - 4:self.current_step, 'Volume'],\n",
        "            self.df.loc[self.current_step -\n",
        "                        4:self.current_step, 'Quote asset volume'],\n",
        "            self.df.loc[self.current_step -\n",
        "                        4:self.current_step, 'Number of trades'],\n",
        "            self.df.loc[self.current_step -\n",
        "                        4:self.current_step, 'Taker buy base asset volume'],\n",
        "            self.df.loc[self.current_step -\n",
        "                        4:self.current_step, 'Taker buy quote asset volume']\n",
        "        ])\n",
        "        # We append additional data\n",
        "        obs = np.append(frame, [[self.balance / MAX_ACCOUNT_BALANCE,\n",
        "                                 self.net_worth / self.max_net_worth,\n",
        "                                 self.crypto_held / MAX_CRYPTO,\n",
        "                                 self.bnb_usdt_held / self.bnb_usdt_held_start,\n",
        "                                 0]],\n",
        "                                axis=0)\n",
        "\n",
        "        return obs\n",
        "\n",
        "    def _take_action(self, action):\n",
        "        # Set the current price to a random price between open and close\n",
        "        current_price = random.uniform(\n",
        "            self.df.loc[self.current_step, 'Real open'],\n",
        "            self.df.loc[self.current_step, 'Real close'])\n",
        "\n",
        "        if action[0] > 0:\n",
        "            # Buy\n",
        "            crypto_bought = self.balance * action[0] / current_price\n",
        "            self.bnb_usdt_held -= crypto_bought * current_price * MAKER_FEE\n",
        "            self.total_fees += crypto_bought * current_price * MAKER_FEE\n",
        "            self.total_volume_traded += crypto_bought * current_price\n",
        "            self.balance -= crypto_bought * current_price\n",
        "            self.crypto_held += crypto_bought\n",
        "\n",
        "        if action[0] < 0:\n",
        "            # Sell\n",
        "            crypto_sold = -self.crypto_held * action[0]\n",
        "            self.bnb_usdt_held -= crypto_sold * current_price * TAKER_FEE\n",
        "            self.total_fees += crypto_sold * current_price * TAKER_FEE\n",
        "            self.total_volume_traded += crypto_sold * current_price\n",
        "            self.balance += crypto_sold * current_price\n",
        "            self.crypto_held -= crypto_sold\n",
        "\n",
        "        self.net_worth = self.balance + self.crypto_held * current_price + self.bnb_usdt_held\n",
        "\n",
        "        if self.net_worth > self.max_net_worth:\n",
        "            self.max_net_worth = self.net_worth\n",
        "\n",
        "    def step(self, action, end=True):\n",
        "        # Execute one time step within the environment\n",
        "        self._take_action(action)\n",
        "\n",
        "        self.current_step += 1\n",
        "\n",
        "        # Calculus of the reward\n",
        "        profit = self.net_worth - (INITIAL_ACCOUNT_BALANCE +\n",
        "                                   BNBUSDTHELD)\n",
        "\n",
        "        profit_percent = profit / (INITIAL_ACCOUNT_BALANCE +\n",
        "                                   BNBUSDTHELD) * 100\n",
        "\n",
        "        benchmark_profit = (self.df.loc[self.current_step, 'Real open'] /\n",
        "                            self.df.loc[self.start_step, 'Real open'] -\n",
        "                            1) * 100\n",
        "\n",
        "        diff = profit_percent - benchmark_profit\n",
        "        reward = np.sign(diff) * (diff)**2\n",
        "\n",
        "        # A single episode can last a maximum of MAX_STEPS steps\n",
        "        if self.current_step >= MAX_STEPS + self.start_step:\n",
        "            end = True\n",
        "        else:\n",
        "            end = False\n",
        "\n",
        "        done = self.net_worth <= 0 or self.bnb_usdt_held <= 0 or end\n",
        "\n",
        "        if done and end:\n",
        "            self.episode_reward = reward\n",
        "            self._render_episode()\n",
        "            self.graph_profit.append(profit_percent)\n",
        "            self.graph_benchmark.append(benchmark_profit)\n",
        "            self.graph_reward.append(reward)\n",
        "            self.episode += 1\n",
        "\n",
        "        obs = self._next_observation()\n",
        "\n",
        "        # {} needed because gym wants 4 args\n",
        "        return obs, reward, done, {}\n",
        "\n",
        "    def render(self, print_step=False, graph=False, *args):\n",
        "        profit = self.net_worth - (INITIAL_ACCOUNT_BALANCE +\n",
        "                                   BNBUSDTHELD)\n",
        "\n",
        "        profit_percent = profit / (INITIAL_ACCOUNT_BALANCE +\n",
        "                                   BNBUSDTHELD) * 100\n",
        "\n",
        "        benchmark_profit = (self.df.loc[self.current_step, 'Real open'] /\n",
        "                            self.df.loc[self.start_step, 'Real open'] -\n",
        "                            1) * 100\n",
        "\n",
        "        if print_step:\n",
        "            print(\"----------------------------------------\")\n",
        "            print(f'Step: {self.current_step}')\n",
        "            print(f'Balance: {round(self.balance, 2)}')\n",
        "            print(f'Crypto held: {round(self.crypto_held, 2)}')\n",
        "            print(f'Fees paid: {round(self.total_fees, 2)}')\n",
        "            print(f'Volume traded: {round(self.total_volume_traded, 2)}')\n",
        "            print(f'Net worth: {round(self.max_net_worth, 2)}')\n",
        "            print(f'Max net worth: {round(self.max_net_worth, 2)}')\n",
        "            print(f'Profit: {round(profit_percent, 2)}% ({round(profit, 2)})')\n",
        "            print(f'Benchmark profit: {round(benchmark_profit, 2)}')\n",
        "\n",
        "        # Plot the graph of the reward\n",
        "        if graph:\n",
        "            fig = plt.figure()\n",
        "            fig.suptitle('Training graph')\n",
        "\n",
        "            high = plt.subplot(2, 1, 1)\n",
        "            high.set(ylabel='Gain')\n",
        "            plt.plot(self.graph_profit, label='Bot profit')\n",
        "            plt.plot(self.graph_benchmark, label='Benchmark profit')\n",
        "            high.legend(loc='upper left')\n",
        "\n",
        "            low = plt.subplot(2, 1, 2)\n",
        "            low.set(xlabel='Episode', ylabel='Reward')\n",
        "            plt.plot(self.graph_reward, label='reward')\n",
        "\n",
        "            plt.show()\n",
        "\n",
        "        return profit_percent, benchmark_profit\n",
        "\n",
        "    def _render_episode(self, filename='render/render.txt'):\n",
        "        file = open(filename, 'a')\n",
        "        file.write('-----------------------\\n')\n",
        "        file.write(f'Episode numero: {self.episode}\\n')\n",
        "        file.write(f'Profit: {round(self.render()[0], 2)}%\\n')\n",
        "        file.write(f'Benchmark profit: {round(self.render()[1], 2)}%\\n')\n",
        "        file.write(f'Reward: {round(self.episode_reward, 2)}\\n')\n",
        "        file.close()"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAexUBKGmCy3"
      },
      "source": [
        "from stable_baselines.common.policies import MlpPolicy\n",
        "from stable_baselines.common.vec_env import DummyVecEnv\n",
        "from stable_baselines import PPO2\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "df = pd.read_csv('data/BTCUSDT.csv', index_col=0)\n",
        "\n",
        "env = DummyVecEnv([lambda: CryptoEnv(df)])\n",
        "\n",
        "# Instanciate the agent\n",
        "model = PPO2(MlpPolicy, env, gamma=1, learning_rate=0.01, verbose=0)\n",
        "\n",
        "# Train the agent\n",
        "total_timesteps = int(os.getenv('TOTAL_TIMESTEPS', 500000))\n",
        "model.learn(total_timesteps)\n",
        "\n",
        "# Render the graph of rewards\n",
        "env.render(graph=True)\n",
        "\n",
        "# Save the agent\n",
        "# model.save('PPO2_CRYPTO')\n",
        "\n",
        "# Trained agent performence\n",
        "obs = env.reset()\n",
        "env.render()\n",
        "for i in range(100000):\n",
        "    action, _states = model.predict(obs)\n",
        "    obs, rewards, done, info = env.step(action)\n",
        "    env.render(print_step=True)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}